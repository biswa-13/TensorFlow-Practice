{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF5 Sentiment Analysis Step By Step .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQl2CQm32nehFoMxrK4w1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biswa-13/TensorFlow-Practice/blob/master/TF5_Sentiment_Analysis_Step_By_Step_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GTVzVninAUl",
        "colab_type": "text"
      },
      "source": [
        "**Step By Step Guid for Doing the Sentiment Analysis using Tensoflow :**\n",
        "- Import the required packages and declaire the variables\n",
        "- Download the dataset\n",
        "- Prepare the training and testing dataset\n",
        "- Tokenizing, Sequencing and Padding the dataset\n",
        "- Create the model\n",
        "- Train the model using the keras Embeddings layer\n",
        "- Predict/Test the model accuracy\n",
        "\n",
        "Reference : https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c04_nlp_embeddings_and_sentiment.ipynb#scrollTo=QXtfw-OY3WoZ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e94GP_6zzPZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "65d31b23-ffb8-4832-dcbb-f1f24d200445"
      },
      "source": [
        "#- Import the required packages and declaire the variables\n",
        "print(\"Start - Import the required packages and declaire the variables\")\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "vocab_size = 1000\n",
        "embeding_dimensions = 16\n",
        "max_len = 100\n",
        "oov_token = \"<OOv>\"\n",
        "trunc_type = \"post\"\n",
        "padding_type = \"post\"\n",
        "\n",
        "print(\"Finish - Import the required packages and declaire the variables\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start - Import the required packages and declaire the variables\n",
            "Finish - Import the required packages and declaire the variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B12UFpv_V5l-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1af721f6-4a32-4f17-87b2-7feb55903f34"
      },
      "source": [
        "# - Downloading the Dataset\n",
        "print(\"Start: Downloading the Dataset -->\")\n",
        "path = tf.keras.utils.get_file(\"reviewDataset.csv\", \"https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\")\n",
        "\n",
        "# Retriving the downloaded datasset\n",
        "dataset = pd.read_csv(path)\n",
        "dataset.head()\n",
        "print(\"Finish: Downloading the Dataset -->\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start: Downloading the Dataset -->\n",
            "Finish: Downloading the Dataset -->\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSVHLW_5Wh4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aa390a27-fa7b-49a6-f531-2df096622985"
      },
      "source": [
        "#- Prepare the training and testing dataset\n",
        "\n",
        "print(\"Start - Prepare the training and testing dataset\")\n",
        "# Get the reviews and lables from the csv file\n",
        "reviews = dataset[\"text\"]\n",
        "labels = dataset[\"sentiment\"]\n",
        "train_size = int(len(reviews) * 0.8)\n",
        "\n",
        "train_dataset = reviews[0: train_size]\n",
        "train_labels = labels[0:train_size]\n",
        "\n",
        "test_dataset = reviews[train_size:]\n",
        "test_labels = labels[train_size:]\n",
        "# Make labels into numpy arrays for use with the network later\n",
        "updtd_train_lbls = np.array(train_labels)\n",
        "updtd_test_lbls = np.array(test_labels)\n",
        "print(\"Finish - Prepare the training and testing dataset\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start - Prepare the training and testing dataset\n",
            "Finish - Prepare the training and testing dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGw8xIrLp4zb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0a009b2d-218a-4580-ef08-165bdbb1c473"
      },
      "source": [
        "# - Tokenize the dataset\n",
        "print(\"Start - Tokenizing, Sequencing and Padding the dataset\")\n",
        "# -- initializing the tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "# -- tokenizing the words\n",
        "tokenizer.fit_on_texts(train_dataset)\n",
        "# -- retriving the wordIndex\n",
        "wordIndex = tokenizer.word_index\n",
        "# -- generating the text sequences\n",
        "sequences_train = tokenizer.texts_to_sequences(train_dataset)\n",
        "# -- applying the paading configurations\n",
        "padding_train = pad_sequences(sequences= sequences_train, maxlen= max_len, padding= padding_type, truncating= trunc_type)\n",
        "\n",
        "# generating sequence and padding for the test dataset\n",
        "sequence_test = tokenizer.texts_to_sequences(test_dataset)\n",
        "padding_test = pad_sequences(sequences= sequence_test, maxlen = max_len, padding= padding_type, truncating= trunc_type)\n",
        "print(\"Finish - Tokenizing, Sequencing and Padding the dataset\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start - Tokenizing, Sequencing and Padding the dataset\n",
            "Finish - Tokenizing, Sequencing and Padding the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UsxbSKKvBOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "27a1f458-5bb1-4fde-f1b5-6fa7c7b95680"
      },
      "source": [
        "# - Building the model using the keras Embeddings layer\n",
        "print(\"Start - Building the model using the keras Embeddings layer\")\n",
        "\n",
        "# Build a basic sentiment network\n",
        "# Note the embedding layer is first, \n",
        "# and the output is only 1 node as it is either 0 or 1 (negative or positive)\n",
        "\n",
        "## initializing the model\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(vocab_size, embeding_dimensions, input_length=max_len),\n",
        "                             tf.keras.layers.Flatten(),\n",
        "                             tf.keras.layers.Dense(6, activation= \"relu\"),\n",
        "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "## compiling the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer= 'adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "print(\"Finish - Building the model using the keras Embeddings layer\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start - Creating and Training the model using the keras Embeddings layer\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 16)           16000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 9606      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 25,613\n",
            "Trainable params: 25,613\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Finish - Creating and Training the model using the keras Embeddings layer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgdA0dXtzFiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b686bf50-285c-4408-ef9b-30e0860b6182"
      },
      "source": [
        "# - Training the model\n",
        "print(\"Start - Training the model\")\n",
        "epochs = 6\n",
        "model.fit(padding_train, updtd_train_lbls, epochs= epochs, validation_data= (padding_test, updtd_test_lbls))\n",
        "print(\"Finish - Training the model\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start - testing the model\n",
            "Epoch 1/6\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5097 - val_loss: 0.6942 - val_accuracy: 0.4110\n",
            "Epoch 2/6\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5229 - val_loss: 0.6963 - val_accuracy: 0.4110\n",
            "Epoch 3/6\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5229 - val_loss: 0.7013 - val_accuracy: 0.4110\n",
            "Epoch 4/6\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5229 - val_loss: 0.7021 - val_accuracy: 0.4110\n",
            "Epoch 5/6\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.6816 - accuracy: 0.5229 - val_loss: 0.6959 - val_accuracy: 0.4110\n",
            "Epoch 6/6\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.5229 - val_loss: 0.6916 - val_accuracy: 0.4110\n",
            "Finish - testing the model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9m4MFf22vGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "a17ed93c-460c-4407-d573-42b79176b50d"
      },
      "source": [
        "# - Testing the model\n",
        "print(\"Start: Testing the model\")\n",
        "# Use the model to predict a review   \n",
        "fake_reviews = ['I love this phone', 'I hate spaghetti', \n",
        "                'Everything was cold',\n",
        "                'Everything was hot exactly as I wanted', \n",
        "                'Everything was green that is good but i dont like the green', \n",
        "                'the host seated us immediately',\n",
        "                'they gave us free chocolate cake', \n",
        "                'not sure about the wilted flowers on the table',\n",
        "                'only works when I stand on tippy toes', \n",
        "                'does not work when I stand on my head']\n",
        "\n",
        "print(fake_reviews) \n",
        "\n",
        "# Create the sequences\n",
        "padding_type='post'\n",
        "sample_sequences = tokenizer.texts_to_sequences(fake_reviews)\n",
        "fakes_padded = pad_sequences(sample_sequences, padding=padding_type, maxlen=max_len)           \n",
        "\n",
        "print('\\nHOT OFF THE PRESS! HERE ARE SOME NEWLY MINTED, ABSOLUTELY GENUINE REVIEWS!\\n')              \n",
        "\n",
        "classes = model.predict(fakes_padded)\n",
        "\n",
        "# The closer the class is to 1, the more positive the review is deemed to be\n",
        "for x in range(len(fake_reviews)):\n",
        "  print(fake_reviews[x])\n",
        "  print(classes[x])\n",
        "  print('\\n')\n",
        "\n",
        "# Try adding reviews of your own\n",
        "# Add some negative words (such as \"not\") to the good reviews and see what happens\n",
        "# For example:\n",
        "# they gave us free chocolate cake and did not charge us\n",
        "print(\"Start: Testing the model\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start: Testing the model\n",
            "['I love this phone', 'I hate spaghetti', 'Everything was cold', 'Everything was hot exactly as I wanted', 'Everything was green that is good but i dont like the green', 'the host seated us immediately', 'they gave us free chocolate cake', 'not sure about the wilted flowers on the table', 'only works when I stand on tippy toes', 'does not work when I stand on my head']\n",
            "\n",
            "HOT OFF THE PRESS! HERE ARE SOME NEWLY MINTED, ABSOLUTELY GENUINE REVIEWS!\n",
            "\n",
            "I love this phone\n",
            "[0.65867996]\n",
            "\n",
            "\n",
            "I hate spaghetti\n",
            "[0.5517792]\n",
            "\n",
            "\n",
            "Everything was cold\n",
            "[0.6035269]\n",
            "\n",
            "\n",
            "Everything was hot exactly as I wanted\n",
            "[0.601652]\n",
            "\n",
            "\n",
            "Everything was green that is good but i dont like the green\n",
            "[0.5872112]\n",
            "\n",
            "\n",
            "the host seated us immediately\n",
            "[0.5933626]\n",
            "\n",
            "\n",
            "they gave us free chocolate cake\n",
            "[0.5982133]\n",
            "\n",
            "\n",
            "not sure about the wilted flowers on the table\n",
            "[0.5296513]\n",
            "\n",
            "\n",
            "only works when I stand on tippy toes\n",
            "[0.60210544]\n",
            "\n",
            "\n",
            "does not work when I stand on my head\n",
            "[0.5013036]\n",
            "\n",
            "\n",
            "Start: Testing the model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}